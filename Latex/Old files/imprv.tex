\chapter{Improvements}\label{ch:improvements}
In this section we will describe in chronological order the various changes
that we applied to the default Kernel Search implementation,
with the objective of improving its performance
in solving the MKP instances.

For the sake of completeness, we will describe all attempts at improving
the algorithm, including the unsuccessful ones.


\section{Variable Sorting}
As outlined in~\cite{kernel:2010} and~\cite{kernel:2012} the quality of
the solutions found by Kernel Search heavily depends on the criterion used to sort the variables.

In ~\cite{kernel:2010}, a sorting criterion for the \textit{Multidimensional Knapsack Problem} is mentioned:
sorting by non-increasing \textit{LP relaxation value} and non-decreasing \textit{absolute value of the reduced cost}.
This is the default criterion that was provided in the code, and
the experiments we executed on the instances proved that this sorting also
provides high quality solutions for the MKP\@.

Other criteria we have tested are:
\begin{itemize}
    \item Sort by non-decreasing \textit{reduced cost}, breaking ties using the non-increasing {LP relaxation value};
    \item Sort by non-increasing \textit{ratio of profit and weight of the item}, breaking ties using the
    non-increasing \textit{reduced cost value};
    \item Sort by non-increasing \textit{value of the LP relaxation} multiplied by the \textit{ratio of profit and
    weight},    breaking ties using the non-decreasing \textit{reduced cost};
    \item Sort randomly.
\end{itemize}

Generally, the efficiency of a sorting criterion depends on the kernel construction criterion used.


\section{Kernel Construction Criterion}\label{sec:kernel_criteria}
A recurring problem we have identified when running complex
instances (such as the one in FK\_4) is that,
from the very beginning, the kernel contains quite a lot of variables,
which makes the solving of the kernel problem and the buckets sub-problems slow.

This means that a starting point to improve the performance of the Kernel Search
could be to optimize the management of the kernel set,
possibly starting from the \textit{kernel construction criterion},
which by default is to include the first \textit{C} sorted variables
(Percentage kernel).

We have experimented with the following criteria:
\begin{enumerate}
    \item Positive kernel: select the variables that have a value greater than 0 in the LP relaxation;
    \item Integer kernel: select the variables that have a value equal to 1 in the LP relaxation;
    \item Threshold kernel: select the variables that have a LP relaxation value greater than a threshold,
    that we have set to 0.6.
\end{enumerate}


\section{Overlapping Buckets}
A criterion that could improve the quality of the solution of the bucket
sub-problems is to use partially overlapping buckets:
this could potentially increase the possibility of related
variables to be included together in the kernel set.

We implemented this feature as an alternative bucket construction criterion.
This feature works well with certain combinations of
kernel construction criterion and variable sorting criterion.


\section{Ejection of Variables from the Kernel}
Another possible improvement for the efficiency of the Kernel Search,
is the \textit{kernel eject} method, which allows to remove variables from the kernel.
This technique is particularly useful for more complex instances, where every
bucket sub-problem normally takes a significant amount of time to be solved.

To implement this functionality, at each iteration of the Kernel Search
we keep a counter \(h\) for the number of solutions found.
Each variable that isn't in the initial kernel
is also associated with a counter \(k_{v}, v \in variables \setminus\) \textit{variables in the initial kernel}
that is incremented when the variable has a non-zero value in the solution of a bucket sub-problem.
At the end of each iteration, for each variable \(v\), if
\((h - k_{v}) - k_{v} <= threshold\), the variable is removed from the kernel.

The values of \(h\) and \(k_{v}\) are reset at each iteration of the Kernel Search.

The \textit{eject threshold} is a configuration parameter:
a low threshold will remove more variables from the kernel, increasing efficiency at the cost of the quality of the
solution found.
A high threshold instead has a more subtle effect.

When the variables are removed from the kernel they are not deleted from the model,
but they are returned to their original bucket.

Experiments on the instances demonstrated that this method significantly cuts
the solving time of the bucket sub-problems.
The quality of the solution found is worsened, but on the flip side
the more iterations and buckets can be solved in the same time limit.


\section{Repetition Counter}
An observation we could make is that when solving buckets,
after a certain number of iterations the algorithm
repeatedly finds new solutions with the same objective value,
and sometimes it even struggles to find new solutions at all.
In other words, the algorithm gets stuck in \textit{local optima},
from which it's hard to escape.

Our hypothesis is that there are two causes for this:
\begin{enumerate}
    \item The GUROBI solver cannot find a (better) solution for the sub-problem,
    either due to the infeasibility of the problem or because of
    the time limit set for solving each bucket.
    \item The Kernel Search algorithm, during its improvement phase,
    only accepts a new solution (thus updating the kernel)
    if it improves upon, or is at least equal, to the incumbent one.
    This is done by adding a cutoff constraint, as explained in~\ref{sec:improving-efficiency}.
\end{enumerate}

In order to mitigate this problem we introduced a \textit{repetition counter}
that, during the improvement phase of Kernel Search,
removes the cutoff constraint for \textit{k} buckets
when the same solution (or no solution) is found for \textit{h} times.

The idea is that this method allows to select if the focus should be on
\textit{diversification} or \textit{intensification},
by appropriately setting parameters \textit{h} and \textit{k}.

A low value for \textit{h} and a high one for \textit{k} allow for
diversification, by adding to the kernel variables that otherwise
may never be selected, and could allow escaping the local optimum.

On the opposite, a high value for \textit{h} and a low one for \textit{k}
switch to focus on intensification, by giving priority to finding
better solutions.

After experimenting with different values for \textit{h} and \textit{k},
we found that keeping them more or less equal allowed for
a reasonable balance between diversification and intensification.
In particular we found that \(h=3\) and \(k=3\) worked quite well
with the test instances we selected.

\subsection{Reset counter on new optimum}
A small enhancement we have applied is to reset the counter to its initial status
whenever the Kernel Search finds a new solution
that is better than any other found before.

This is significant because the default repetition counter
method completely ignores the value of the solutions found
during the \textit{k} cycles.


\section{Item Dominance}
In~\cite{mkp:2019} an improvement for the MKP model is suggested:
given two items \textit{j} and \textit{k}, if \(w_{j} \leq w_{k}\)
and \(p_{j} \geq p_{k}\), then it is said that \textit{j dominates k}.
This means that when an item is excluded from the solution,
all items dominated by it can also be excluded.

The simplest way to implement this method is to preliminarily sort all
items according to non-increasing weight, breaking ties by
non-decreasing profit.
For each item \(k\), items \(j \coloneqq k+1,\dots,n\) are parsed,
and if \(p_{j} \geq p_{k}\) then the pair \((j,k)\) is added to a
dominance list \(D\).
Then, the following constraints are added to the model:
\begin{equation}
    \label{eq:itemdom}
    \sum_{i=1}^{m} x_{ij} \geq \sum_{i=1}^{m} x_{ik} \qquad (j,k) \in D
\end{equation}

To efficiently implement this technique,the dominance list is only built once at startup
and the constraints~\eqref{eq:itemdom} are applied when solving
the relaxation, the kernel problem and the bucket sub-problems.

The testing proved that this method increases performance of the Kernel Search with very little
overhead costs.


\section{Instance Reduction}
Another improvement for the MKP model describe in~\cite{mkp:2019} is
\textit{instance reduction}.

Let \textit{I} be any subset of knapsacks and let \textit{J} be the set of
all items of the instance that can be packed in a knapsack of \textit{I}:
\begin{equation}
    J \coloneqq \{j:w_{j} \leq \max_{i \in I} \{c_{i}\}, 1 \leq j \leq n\}
\end{equation}
If there exists a feasible packing of the items of \textit{J} into the knapsacks
of \textit{I}, then such packing can be fixed, and sets \textit{I} and \textit{J}
can be removed from the instance.

The most efficient way to implement this property is to start
by sorting the knapsacks by non-decreasing capacity, add the first knapsack
to set \textit{I}, and iteratively add the next (smaller) knapsack to \textit{I}.
At each iteration, we add the appropriate items to \textit{J},
and check if \(\sum_{j \in J} w_{j} \leq \sum_{i \in I} c_{i}\).
This allows to avoid running the more expensive bin packing algorithm
when it's guaranteed that there is no feasible solution.

To solve the bin packing problem, in~\cite{mkp:2019} an exact method is suggested.
However, the execution time for such an algorithm is unacceptable for this project.
We decided anyway to try solving the bin packing problem using a modified
version of the \textit{First-Fit-Decreasing (FFD)} heuristic.

The basic version of FFD works as follows:
Given \textit{n} items with a weight and a fixed capacity for each bin:
\begin{enumerate}
    \item Order the items from largest to smallest;
    \item Open a new empty bin;
    \item For each item, from largest to smallest, find the first bin into which the item fits, if any.
    If such a bin is found, put the item in it.
    Otherwise, open a new empty bin and put the item in it.
\end{enumerate}

In this algorithm the bins correspond to the knapsacks of \textit{I},
and the items are the ones contained in \textit{J}.
Also, the number of bins is pre-determined by the size of \textit{I},
and each bin has a different capacity (corresponding to the capacity of the knapsack).

We implemented this algorithm in the code as an additional step to be run
before solving the kernel problem and the bucket sub-problems.
The testing however revealed that the algorithm couldn't find any valid bin packing
for any test instance.
After some consideration, we supposed that even if we were to swap FFD with another
heuristic algorithm, it would be unlikely that the bin packing found (if any) would impact the efficiency of the
Kernel Search by much.
Because of this reason, we decided to discard this technique.


\section{Single Knapsack Heuristic}
As an experiment, we tried to implement a heuristic algorithm that,
for each knapsack, solves a \textit{0-1 knapsack problem},
which finds the optimum items to be included in that knapsack.

Given \(n\) items with a weight \(w_{j}\) and a profit \(p_{j}\),
and a knapsack with capacity \(c\), the \textit{0-1 Knapsack Problem} finds
the subset of items that maximizes the overall profit while not overflowing the capacity of the knapsack.
\begin{equation}
    \max{z} = \sum_{j=1}^{n} p_{j} x_{j}
\end{equation}
\begin{equation}
    \sum_{j=1}^{n} w_{j} x_{j} \leq c
\end{equation}
\begin{equation}
    x_{j} \in \{0,1\} \qquad j=1,\dots,n
\end{equation}
It's worth noting that the \textit{0-1 Knapsack Problem} is equal to an MKP
where \(m=1\);

The detailed algorithm begins with defining \(J \coloneqq \text{items of the MKP}\)
and \(X \coloneqq \emptyset\) as the (initially empty) solution found by the heuristic.
Then, for each knapsack \(i = 1,\dots,m\) the following steps are repeated:
\begin{enumerate}
    \item Solve the \textit{0-1 Knapsack Problem} that uses \(i\) as the knapsack and the items in \(J\);
    \item Remove the items that were inserted into the knapsack from \(J\);
    \item Add the solution found (associated to the knapsack) to \(X\);
\end{enumerate}
Since every item can at most be inserted into one knapsack, and the capacity of each
knapsack is respected, \(X\) is guaranteed to be a feasible solution for the MKP\@.

After preliminarily testing the heuristic by itself (outside the Kernel Search framework)
we found that the results were excellent, and required very little computation time.

\subsection{Integration with the Kernel Search}
To integrate the heuristic with the Kernel Search, our idea was to use the heuristic
to find a high quality starting solution, which the Kernel Search tries to improve upon.

To achieve this, we removed the solving of the LP relaxation, and instead
used the solution found by the heuristic to build the kernel set and the buckets.
The parameters of the Kernel Search and the methods added until now
did not require any change to work correctly.

It is possible that the Kernel Search does not find a solution that improves
upon the one found by the heuristic: this is still acceptable, because the
heuristic finds integer and feasible solutions.


